# ğŸ¤– Lab 01 - Understanding AI Agents

| ğŸ“‹ Attribute | Value |
|-------------|-------|
| â±ï¸ **Duration** | 90 minutes |
| ğŸ“Š **Difficulty** | â­â­ Intermediate |
| ğŸ¯ **Prerequisites** | Lab 00 completed |

---

## ğŸ“ˆ Progress Tracker

```
Lab Progress: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% - Not Started

Checkpoints:
â–¡ Understand Multi-Agent Architecture
â–¡ Learn QueryAgent Pattern
â–¡ Learn RouterAgent Pattern
â–¡ Learn ActionAgent Pattern
â–¡ Complete Exercise 1a: Intent Classification
â–¡ Complete Exercise 1b: Prompt Engineering
â–¡ Achieve >90% Intent Accuracy
â–¡ Complete Self-Assessment
```

---

## ğŸ¯ Learning Objectives

By the end of this lab, you will be able to:

1. ğŸ—ï¸ **Understand multi-agent vs single agent architectures** - Compare the trade-offs between monolithic AI systems and distributed agent patterns
2. ğŸ”„ **Learn the QueryAgent â†’ RouterAgent â†’ ActionAgent pattern** - Master the three-agent architecture used throughout this accelerator
3. ğŸ¯ **Practice intent classification** - Build and test an intent classifier that routes user queries to the appropriate agent

---

## ğŸ¤” Why Multi-Agent Over Monolithic?

When building AI-powered applications, a natural first instinct is to create a single, powerful agent that handles everything. While this works for simple use cases, it quickly becomes problematic as complexity grows.

### âŒ The Monolithic Problem

A single-agent architecture suffers from:

| ğŸš« Issue | ğŸ“ Description |
|---------|-------------|
| **Prompt Bloat** | System prompts grow unwieldy as you add more capabilities |
| **Context Confusion** | The model struggles to stay focused when handling disparate tasks |
| **Testing Difficulty** | Hard to isolate and test specific behaviors |
| **Maintenance Burden** | Changes to one capability risk breaking others |
| **Cost Inefficiency** | Every request pays for the full prompt, even for simple tasks |

### âœ… The Multi-Agent Solution

Breaking your system into specialized agents provides:

- ğŸ¯ **Separation of Concerns** - Each agent has a clear, bounded responsibility
- ğŸ“ **Focused Prompts** - Shorter, more precise instructions lead to better outputs
- ğŸ§ª **Independent Testing** - Test each agent in isolation
- ğŸ“ˆ **Selective Scaling** - Route simple queries to cheaper/faster models
- ğŸ” **Easier Debugging** - Trace issues to specific agents in the pipeline

---

## ğŸ”„ The Three-Agent Pattern

This accelerator uses a proven three-agent architecture that balances simplicity with power:

```
                                    +------------------+
                                    |   ActionAgent    |
                                    |   (RAG Search)   |
                                    +------------------+
                                           ^
                                           |
+------------------+    +------------------+
|   QueryAgent     |--->|   RouterAgent    |
| (Understanding)  |    |  (Dispatching)   |
+------------------+    +------------------+
        ^                      |
        |                      v
   User Query           +------------------+
                        |   ActionAgent    |
                        |   (API Call)     |
                        +------------------+
                               |
                               v
                        +------------------+
                        |   ActionAgent    |
                        | (Conversation)   |
                        +------------------+
```

### ğŸ‘¥ Agent Responsibilities

#### ğŸ” QueryAgent - The Understander

**Purpose:** Transform raw user input into structured, actionable data.

**Responsibilities:**
- ğŸ“ Parse natural language queries
- ğŸ·ï¸ Extract entities (names, dates, amounts, etc.)
- âœ¨ Normalize input (fix typos, expand abbreviations)
- ğŸ¯ Identify query type and intent
- ğŸ“š Enrich context with conversation history

| ğŸ“¥ Input | ğŸ“¤ Output |
|----------|----------|
| Raw user message + conversation context | Structured query object with intent, entities, and metadata |

#### ğŸš¦ RouterAgent - The Dispatcher

**Purpose:** Determine the best action path for a given query.

**Responsibilities:**
- ğŸ·ï¸ Classify intent into predefined categories
- âœ… Select appropriate ActionAgent(s)
- â“ Handle ambiguous queries (ask for clarification)
- ğŸ“‹ Apply business rules and access control
- ğŸ“Š Log routing decisions for analytics

| ğŸ“¥ Input | ğŸ“¤ Output |
|----------|----------|
| Structured query from QueryAgent | Routing decision with selected agent(s) and parameters |

#### âš¡ ActionAgent(s) - The Doers

**Purpose:** Execute specific tasks and generate responses.

**Types of ActionAgents:**
- ğŸ“š **RAG Agent** - Searches knowledge bases and synthesizes answers
- ğŸ”— **API Agent** - Calls external services and transforms responses
- ğŸ’¬ **Conversation Agent** - Handles chitchat and clarifications
- ğŸ“‹ **Task Agent** - Performs multi-step workflows

| ğŸ“¥ Input | ğŸ“¤ Output |
|----------|----------|
| Routing decision with parameters | Final response to user |

---

## ğŸ›¡ï¸ Agent Boundaries and Responsibilities

Clear boundaries between agents are critical. Here is a decision matrix:

| ğŸ“‹ Concern | ğŸ” QueryAgent | ğŸš¦ RouterAgent | âš¡ ActionAgent |
|---------|------------|-------------|-------------|
| Parse user input | âœ… Yes | âŒ No | âŒ No |
| Classify intent | ğŸ”¶ Partial | âœ… Yes | âŒ No |
| Select execution path | âŒ No | âœ… Yes | âŒ No |
| Call external APIs | âŒ No | âŒ No | âœ… Yes |
| Generate final response | âŒ No | âŒ No | âœ… Yes |
| Maintain conversation state | âœ… Yes | âŒ No | ğŸ”¶ Partial |

### ğŸš« Anti-Patterns to Avoid

1. âŒ **Router doing understanding** - Keep parsing in QueryAgent
2. âŒ **ActionAgent re-classifying** - Trust the router's decision
3. âŒ **QueryAgent calling APIs** - It should only understand, not act
4. âŒ **Circular dependencies** - Agents should not call back to earlier stages

---

## ğŸ“ Exercises

Complete the following hands-on exercises to reinforce your understanding:

### ğŸ“š Exercise 1a: Intent Classification
**File:** [exercises/01a-intent-classification.md](exercises/01a-intent-classification.md)

Build an intent classifier that categorizes user queries into predefined intents. You will:
- ğŸ·ï¸ Define intent categories for your domain
- ğŸ“ Create training examples for each intent
- ğŸ’» Implement classification logic
- ğŸ§ª Test against edge cases

### âœï¸ Exercise 1b: Prompt Engineering
**File:** [exercises/01b-prompt-engineering.md](exercises/01b-prompt-engineering.md)

Craft effective prompts for each agent in the pipeline. You will:
- ğŸ” Write a QueryAgent system prompt
- ğŸš¦ Design a RouterAgent decision prompt
- âš¡ Create ActionAgent task prompts
- ğŸ”„ Test prompt variations

---

## âœ… Deliverables

By the end of this lab, you should have:

| ğŸ“‹ Deliverable | âœ… Success Criteria |
|-------------|------------------|
| ğŸ¯ Intent Classifier | >90% accuracy on test queries |
| ğŸ“ Agent Prompts | Three working prompts (Query, Router, Action) |
| ğŸ—ï¸ Architecture Diagram | Your own version showing data flow |
| ğŸ§ª Test Suite | At least 20 test queries with expected classifications |

---

## ğŸ”§ Troubleshooting Tips

### âš ï¸ Common Issues

**Issue:** Intent classifier accuracy below 90%
- âœ… **Solution:** Review misclassified examples and add them to training data
- âœ… **Solution:** Check for overlapping intent definitions - make categories more distinct
- âœ… **Solution:** Add more diverse examples per intent (aim for 10+ each)

**Issue:** RouterAgent selecting wrong ActionAgent
- âœ… **Solution:** Verify QueryAgent is extracting correct intent
- âœ… **Solution:** Review routing rules for gaps or conflicts
- âœ… **Solution:** Add explicit handling for edge cases

**Issue:** Prompts generating inconsistent outputs
- âœ… **Solution:** Add output format examples to your prompts
- âœ… **Solution:** Use structured output (JSON) where possible
- âœ… **Solution:** Lower temperature for more deterministic results

**Issue:** Context getting lost between agents
- âœ… **Solution:** Ensure conversation history is passed through pipeline
- âœ… **Solution:** Check that entity extraction preserves all relevant data
- âœ… **Solution:** Add logging to trace data flow between agents

### ğŸ“‹ Debugging Checklist

1. [ ] âœ… Verify Lab 00 setup is complete and working
2. [ ] ğŸ”‘ Check that all API keys/endpoints are configured
3. [ ] ğŸ“ Enable verbose logging for each agent
4. [ ] ğŸ§ª Test each agent in isolation before testing the pipeline
5. [ ] ğŸ” Compare actual vs expected outputs at each stage
6. [ ] âš ï¸ Review Azure OpenAI rate limits if seeing throttling

### ğŸ†˜ Getting Help

- ğŸ“– Review the architecture diagram above
- ğŸ“„ Check the exercise files for hints
- ğŸ“š Consult the main accelerator documentation
- ğŸ‘‹ Reach out to your instructor or lab assistant

---

## ğŸ§  Check Your Understanding

Before proceeding to Lab 02, verify you can answer these questions:

### ğŸ“ Concept Check

| â“ Question | ğŸ“ Your Answer |
|----------|-------------|
| Why is a multi-agent architecture better than a monolithic agent for complex systems? | _[Write your answer]_ |
| What is the primary responsibility of the QueryAgent? | _[Write your answer]_ |
| When should the RouterAgent escalate to a human instead of dispatching to an ActionAgent? | _[Write your answer]_ |
| What's the difference between a RAG ActionAgent and an API ActionAgent? | _[Write your answer]_ |

### âœ… Self-Assessment Checklist

Complete this checklist to confirm you're ready for Lab 02:

- [ ] ğŸ—£ï¸ I can explain the three-agent pattern to someone new
- [ ] ğŸ¯ My intent classifier achieves >90% accuracy on test queries
- [ ] âœï¸ I have written prompts for QueryAgent, RouterAgent, and ActionAgent
- [ ] ğŸ”„ I understand why agents should NOT call back to earlier stages in the pipeline
- [ ] ğŸ¨ I can draw an architecture diagram showing data flow between agents
- [ ] ğŸ§ª I have at least 20 test queries with expected classifications

### ğŸ§ª Quick Quiz

Test yourself with these scenarios:

1. **Scenario:** A user asks "What's my financial aid status?" and also mentions "my password isn't working"
   - ğŸ” Which agent handles parsing both issues?
   - ğŸš¦ How should the RouterAgent handle multiple intents?

2. **Scenario:** The QueryAgent extracts an intent but with only 60% confidence
   - â“ Should the RouterAgent proceed or ask for clarification?
   - ğŸ“Š What factors influence this decision?

3. **Scenario:** An ActionAgent needs information that wasn't in the original query
   - ğŸ”„ Can it call back to QueryAgent to re-parse?
   - âœ… What's the correct pattern to handle this?

**Answers:** Discuss with your coach or check [coach-guide/TALKING_POINTS.md](../../coach-guide/TALKING_POINTS.md) for guidance.

---

## â¡ï¸ Next Steps

Once you have completed this lab and achieved >90% accuracy on your intent classifier, proceed to:

**[Lab 02 - Azure MCP Setup](../02-azure-mcp-setup/README.md)** â˜ï¸

In the next lab, you will configure Azure OpenAI and Azure AI Search services to power your agents.

---

## ğŸ“Š Version Matrix

| Component | Required Version | Tested Version |
|-----------|-----------------|----------------|
| ğŸ Python | 3.11+ | 3.11.9 |
| ğŸ¤– Azure OpenAI | GPT-4o | 2024-02-15-preview |
| ğŸ”§ Pydantic | 2.5+ | 2.6.1 |
| ğŸ“ Prompt Engineering | N/A | Best practices |

---

<div align="center">

[â† Lab 00](../00-setup/README.md) | **Lab 01** | [Lab 02 â†’](../02-azure-mcp-setup/README.md)

ğŸ“… Last Updated: 2026-02-04 | ğŸ“ Version: 1.0.0

</div>
